{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65d43f7c-6d05-4a7e-bad5-9baac26274a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/04 02:13:29 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception happened during processing of request from ('127.0.0.1', 55682)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.8/socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/usr/lib/python3.8/socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/usr/lib/python3.8/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/usr/lib/python3.8/socketserver.py\", line 747, in __init__\n",
      "    self.handle()\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pyspark/accumulators.py\", line 262, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pyspark/accumulators.py\", line 235, in poll\n",
      "    if func():\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pyspark/accumulators.py\", line 239, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pyspark/serializers.py\", line 564, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import time\n",
    "import csv\n",
    "import math\n",
    "import numpy as np\n",
    "import pyspark\n",
    "from pyspark import SparkContext, SparkConf\n",
    "import json\n",
    "from operator import add\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from node2vec import Node2Vec as n2v\n",
    "import networkx as nx\n",
    "from hashlib import md5\n",
    "import pickle\n",
    "import os\n",
    "import pandas as pd\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 100)\n",
    "pd.set_option('display.width', 100)\n",
    "pd.set_option(\"display.precision\", 2)\n",
    "\n",
    "\n",
    "from utils import create_category_md5_mapping, integrate_mapping_user_bus_cat_data, dataframe_to_rdd_dict, analyze_top_business_categories, analyze_top_categories\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.mllib.recommendation import ALS, MatrixFactorizationModel, Rating\n",
    "\n",
    "def initialize_spark_context(APP_NAME=\"Train: XGBModel\"):\n",
    "    # Spark配置项列表\n",
    "    SPARK_CONF = [\n",
    "        (\"spark.dynamicAllocation.enabled\", \"true\"),  # 启用动态资源分配\n",
    "        (\"spark.dynamicAllocation.maxExecutors\", \"10\"),  # 最大执行器数量\n",
    "        (\"spark.executor.memory\", \"3g\"),  # 每个执行器的内存\n",
    "        (\"spark.executor.cores\", \"2\"),  # 每个执行器的CPU核心数\n",
    "        (\"spark.executor.memoryOverhead\", \"3000\"),  # 执行器内存开销\n",
    "        (\"spark.driver.memory\", \"4g\"),  # 驱动程序的内存\n",
    "        (\"spark.driver.maxResultSize\", \"2g\"),  # 驱动程序的最大结果大小\n",
    "        (\"spark.python.worker.memory\", \"2g\"),  # Python工作进程的内存\n",
    "        (\"spark.sql.shuffle.partitions\", \"20\"),  # Shuffle操作的分区数\n",
    "        (\"spark.sql.sources.partitionOverWriteMode\", \"dynamic\"),  # 分区覆写模式\n",
    "        (\"spark.network.timeout\", \"600s\"),  # 网络超时设置\n",
    "        (\"spark.executor.heartbeatInterval\", \"120s\"),  # 执行器心跳间隔\n",
    "    ]\n",
    "\n",
    "    # 创建Spark配置\n",
    "    spark_conf = pyspark.SparkConf()\n",
    "    spark_conf.setAppName(APP_NAME)\n",
    "    spark_conf.setAll(SPARK_CONF)\n",
    "\n",
    "    # 创建SparkContext\n",
    "    sc = pyspark.SparkContext(conf=spark_conf)\n",
    "    sc.setLogLevel(\"ERROR\")  # 设置日志级别\n",
    "\n",
    "    return sc\n",
    "\n",
    "sc = initialize_spark_context()\n",
    "\n",
    "import better_features\n",
    "from better_features import FeatureProcessor, read_json_data, transform_user_data, transform_business_data, extract_review_data\n",
    "from datetime import datetime #add\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "def rdd_to_pandas(rdd):\n",
    "    return pd.DataFrame(rdd.collect(), columns=rdd.first().keys())\n",
    "\n",
    "def prepare_test_data(test_df, cluster):\n",
    "    if cluster == -1:\n",
    "        test_cluster_data = test_df\n",
    "    else:\n",
    "        test_cluster_data = test_df[test_df['Cluster'] == cluster]\n",
    "        \n",
    "    X_test = test_cluster_data.drop(columns=['stars', 'user_id', 'business_id'])\n",
    "    y_test = test_cluster_data['stars']\n",
    "    return X_test, y_test\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "user_parsed_df = pd.read_csv('cache/user_df.csv') # parsed from users.json\n",
    "business_parsed_df = pd.read_csv('cache/business_df.csv') # parsed from business.json\n",
    "review_parsed_df = pd.read_csv('cache/review_df.csv') # parsed from business.json\n",
    "\n",
    "train_df = pd.read_csv('cache/train_df.csv', index_col=None)\n",
    "test_df = pd.read_csv('cache/test_df.csv', index_col=None)\n",
    "val_df = pd.read_csv('cache/val_df.csv', index_col=None)\n",
    "\n",
    "print(\"DONE!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6342a49e-8b1e-4243-89d5-a24818213475",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n"
     ]
    }
   ],
   "source": [
    "# from better_features import FeatureProcessor, read_json_data, transform_user_data, transform_business_data,extract_review_data\n",
    "# from utils import integrate_mapping_user_bus_cat_data\n",
    "# from KMeans_user_cluster import KMeans_process_user_clusters\n",
    "\n",
    "# data_folder_path, test_data_file, output_file = '../data/', '../data/yelp_true.csv', 'prediction.csv'\n",
    "# feature_processor = FeatureProcessor(sc, data_folder_path, user_parsed_df, business_parsed_df, review_parsed_df)\n",
    "# user_clusters = KMeans_process_user_clusters(feature_processor.map_reviews_with_categories(), business_parsed_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "284e518a-78ff-4f7c-9d85-760942c9dddf",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE!\n"
     ]
    }
   ],
   "source": [
    "# ## TRAIN DATA FEATURE PROCESSING HERE\n",
    "# train_data_file = '../data/yelp_train.csv'\n",
    "# train_df = pd.read_csv(train_data_file)\n",
    "# train_df = feature_processor.process_all_features(sc, train_df, train_data_file)\n",
    "# train_df = train_df.merge(user_clusters, on='user_id', how='left')\n",
    "\n",
    "# print(\"DONE!\")\n",
    "\n",
    "# ## TEST DATA FEATURE PROCESSING HERE\n",
    "# val_data_file = '../data/yelp_val.csv'\n",
    "# val_df = pd.read_csv(val_data_file)\n",
    "# val_df = feature_processor.process_all_features(sc, val_df, test_data_file)\n",
    "# val_df = val_df.merge(user_clusters, on='user_id', how='left')\n",
    "\n",
    "# print(\"DONE!\")\n",
    "\n",
    "# ## TEST DATA FEATURE PROCESSING HERE\n",
    "# test_data_file = '../data/yelp_true.csv'\n",
    "# test_df = pd.read_csv(test_data_file)\n",
    "# test_df = feature_processor.process_all_features(sc, test_df, test_data_file)\n",
    "# test_df = test_df.merge(user_clusters, on='user_id', how='left')\n",
    "\n",
    "# print(\"DONE!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a4a6c1f-bbf5-4ef9-a9ac-6c1d446b93d2",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE!\n"
     ]
    }
   ],
   "source": [
    "# train_df.to_csv('archived/train_df.csv', index=False)\n",
    "# test_df.to_csv('archived/test_df.csv', index=False)\n",
    "# val_df.to_csv('archived/val_df.csv', index=False)\n",
    "\n",
    "# print(\"DONE!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e977320a-1464-4bf2-9e5a-bcac620ac425",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3171185c-8bd3-4cd6-9dfa-bee0226cf469",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALL use early_stopping_rounds=10\n",
    "\n",
    "# 0, 2, 3, 4, \n",
    "\n",
    "# 0,2 no splitting val, \n",
    "# 3,4 using only catboost\n",
    "# test_size = 0.05\n",
    "large_xgb_params = {\n",
    "    0: {'learning_rate': 0.04246778091879101, 'max_depth': 6, 'n_estimators': 491, 'subsample': 0.8689},\n",
    "    2: {'learning_rate': 0.12350751460907078, 'max_depth': 3, 'n_estimators': 288, 'subsample': 0.8880},\n",
    "    3: {'learning_rate': 0.08288064461501718, 'max_depth': 5, 'n_estimators': 305, 'subsample': 0.6839},\n",
    "    4: {'learning_rate': 0.18299396047960448, 'max_depth': 4, 'n_estimators': 250, 'subsample': 0.9081}\n",
    "}\n",
    "\n",
    "large_catboost_params = {\n",
    "    0: {'depth': 12, 'l2_leaf_reg': 18.15, 'learning_rate': 0.06229, 'n_estimators': 1486},\n",
    "    2: {'depth': 6, 'l2_leaf_reg': 28.02, 'learning_rate': 0.1090, 'n_estimators': 176},\n",
    "    3: {'depth': 5, 'l2_leaf_reg': 0.2358, 'learning_rate': 0.1968, 'n_estimators': 62},\n",
    "    4: {'depth': 4, 'l2_leaf_reg': 0.3376, 'learning_rate': 0.1914, 'n_estimators': 46}\n",
    "}\n",
    "\n",
    "# 5, 6, 7, 8\n",
    "# test_size = 0.1\n",
    "medium_catboost_params = {\n",
    "    5: {'depth': 3, 'l2_leaf_reg': 0.415375550656062, 'learning_rate': 0.9017553809036529}, \n",
    "    7: {'depth': 6, 'l2_leaf_reg': 18.3792029910461, 'learning_rate': 0.9372003134293689}, \n",
    "    8: {'depth': 1, 'l2_leaf_reg': 7.440376345058953, 'learning_rate': 0.4049807340854126},\n",
    "    6: {'depth': 2, 'l2_leaf_reg': 1.1835517768726047, 'learning_rate': 0.38770112704134657}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85996661-5186-46a5-8b21-b421cfca4c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def prepare_data(df, cluster, drop_cols):\n",
    "#     data = df[df['Cluster'] == cluster]\n",
    "#     X = data.drop(columns=drop_cols)\n",
    "#     y = data['stars']\n",
    "#     return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "966012fc-5d9f-4d81-bc70-1ba1a4ae5479",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from catboost import CatBoostRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.linear_model import ElasticNet, ElasticNetCV\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "def prepare_data(df, cluster, drop_cols):\n",
    "    if cluster == -1:\n",
    "        cluster_data = df\n",
    "    else:\n",
    "        cluster_data = df[df['Cluster'] == cluster]\n",
    "\n",
    "    drop_cols = [col for col in drop_cols if col in cluster_data.columns]\n",
    "    X = cluster_data.drop(columns=drop_cols, errors='ignore')\n",
    "    y = cluster_data['stars']\n",
    "    return X, y\n",
    "\n",
    "def train_model(cluster, train_df, test_df):\n",
    "    if cluster in [5]:\n",
    "            drop_cols = ['stars', 'user_id', 'business_id', 'log_affinity_score']  # For cluster 5, do not drop 'score'\n",
    "    elif cluster in [7, 8]:\n",
    "        drop_cols = ['stars', 'user_id', 'business_id', 'score', 'log_affinity_score']  # Drop both 'score' and 'log_affinity_score'\n",
    "    else:\n",
    "        drop_cols = ['stars', 'user_id', 'business_id']  # Default columns to drop\n",
    "    \n",
    "    if cluster in [0, 2]:\n",
    "        X_train, y_train = prepare_data(train_df, cluster, drop_cols)\n",
    "        X_test, y_test = prepare_data(test_df, cluster, drop_cols)  \n",
    "\n",
    "        cb_model = CatBoostRegressor(**large_catboost_params[cluster], verbose=False)\n",
    "        xgb_model = XGBRegressor(**large_xgb_params[cluster], objective='reg:squarederror', verbosity=0)\n",
    "        \n",
    "        cb_model.fit(X_train, y_train)\n",
    "        xgb_model.fit(X_train, y_train)\n",
    "        \n",
    "        cb_preds = cb_model.predict(X_test)\n",
    "        xgb_preds = xgb_model.predict(X_test)\n",
    "        \n",
    "        # Applying best weight ratio and clipping\n",
    "        final_preds = (0.30 * cb_preds + 0.70 * xgb_preds) if cluster == 0 else (0.65 * cb_preds + 0.35 * xgb_preds)\n",
    "        final_preds = np.clip(final_preds, 1, 5)\n",
    "    elif cluster in [3, 4, 5, 6, 7, 8]:\n",
    "        test_size = 0.05 if cluster in [3, 4] else 0.10\n",
    "        X_train, y_train = prepare_data(train_df, cluster, drop_cols)\n",
    "        X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=test_size, random_state=42)\n",
    "        X_test, y_test = prepare_data(test_df, cluster, drop_cols)\n",
    "\n",
    "        model = CatBoostRegressor(**(large_catboost_params[cluster] if cluster in [3, 4] else medium_catboost_params[cluster]), verbose=False)\n",
    "        model.fit(X_train, y_train, eval_set=(X_val, y_val), early_stopping_rounds=10)\n",
    "        final_preds = model.predict(X_test).clip(1, 5)\n",
    "    elif cluster == 1:\n",
    "        if pd.isnull(train_df['score']).any() or pd.isnull(test_df['score']).any():\n",
    "            drop_cols.append('score')\n",
    "            \n",
    "        X_train, y_train = prepare_data(train_df, cluster, drop_cols)\n",
    "        X_test, y_test = prepare_data(test_df, cluster, drop_cols)\n",
    "\n",
    "        # model = ElasticNet(**small_ES_params[cluster])\n",
    "        model = ElasticNetCV(l1_ratio=[.1, .5, .7, .9, .95, .99, 1], alphas=np.logspace(-6, 2, 100), cv=5, random_state=42)\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        model = ElasticNet(alpha=model.alpha_, l1_ratio=model.l1_ratio_)\n",
    "        model.fit(X_train, y_train)\n",
    "        final_preds = model.predict(X_test).clip(1, 5)\n",
    "\n",
    "    \n",
    "    rmse = np.sqrt(mean_squared_error(y_test, final_preds))\n",
    "    return rmse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df17cb1e-128c-4a63-8c53-80b03f640c28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8149a916-b069-41b0-be2c-2243114274ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2622b96-0d7e-4a8b-9b17-2ccc642da1c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5be27139-638a-4e1c-bc44-f46ad15f9b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from catboost import CatBoostRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.linear_model import ElasticNet, ElasticNetCV\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "\n",
    "def find_best_weight(cluster, X_train, y_train, X_test, y_test):\n",
    "    # init\n",
    "    cb_model = CatBoostRegressor(**large_catboost_params[cluster], verbose=False)\n",
    "    xgb_model = XGBRegressor(**large_xgb_params[cluster], objective='reg:squarederror', verbosity=0)\n",
    "\n",
    "    # train\n",
    "    cb_model.fit(X_train, y_train)\n",
    "    xgb_model.fit(X_train, y_train)\n",
    "\n",
    "    # predict\n",
    "    cb_preds = cb_model.predict(X_test).clip(1, 5)\n",
    "    xgb_preds = xgb_model.predict(X_test).clip(1, 5)\n",
    "\n",
    "    best_rmse = float('inf')\n",
    "    best_ratio = 0\n",
    "    \n",
    "    for ratio in np.linspace(0, 1, 21):  # test different porpotion\n",
    "        final_preds = ratio * cb_preds + (1 - ratio) * xgb_preds\n",
    "        rmse = np.sqrt(mean_squared_error(y_test, final_preds))\n",
    "        if rmse < best_rmse:\n",
    "            best_rmse = rmse\n",
    "            best_ratio = ratio\n",
    "\n",
    "    return best_ratio, best_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "fb868d2f-2adb-4c51-81c7-fe818fb4e098",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from catboost import CatBoostRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.linear_model import ElasticNet, ElasticNetCV\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "\n",
    "def find_best_weight(cluster, X_train, y_train, X_test, y_test):\n",
    "    # init\n",
    "    cb_model = CatBoostRegressor(**large_catboost_params[cluster], verbose=False)\n",
    "    xgb_model = XGBRegressor(**large_xgb_params[cluster], objective='reg:squarederror', verbosity=0)\n",
    "\n",
    "    # train\n",
    "    cb_model.fit(X_train, y_train)\n",
    "    xgb_model.fit(X_train, y_train)\n",
    "\n",
    "    # predict\n",
    "    cb_preds = cb_model.predict(X_test).clip(1, 5)\n",
    "    xgb_preds = xgb_model.predict(X_test).clip(1, 5)\n",
    "\n",
    "    best_rmse = float('inf')\n",
    "    best_ratio = 0\n",
    "    \n",
    "    for ratio in np.linspace(0, 1, 21):  # test different porpotion\n",
    "        final_preds = ratio * cb_preds + (1 - ratio) * xgb_preds\n",
    "        rmse = np.sqrt(mean_squared_error(y_test, final_preds))\n",
    "        if rmse < best_rmse:\n",
    "            best_rmse = rmse\n",
    "            best_ratio = ratio\n",
    "\n",
    "    return best_ratio, best_rmse\n",
    "\n",
    "\n",
    "def train_model(cluster, train_df, test_df):\n",
    "    def prepare_data(df, cluster, drop_cols):\n",
    "        if cluster == -1:\n",
    "            cluster_data = df\n",
    "        else:\n",
    "            cluster_data = df[df['Cluster'] == cluster]\n",
    "    \n",
    "        drop_cols = [col for col in drop_cols if col in cluster_data.columns]\n",
    "        X = cluster_data.drop(columns=drop_cols, errors='ignore')\n",
    "        y = cluster_data['stars']\n",
    "        return X, y\n",
    "        \n",
    "    if cluster in [5]:\n",
    "        drop_cols = ['stars', 'user_id', 'business_id', 'log_affinity_score']  # For cluster 5, do not drop 'score'\n",
    "    elif cluster in [6, 7, 8]:\n",
    "        drop_cols = ['stars', 'user_id', 'business_id', 'binary_affinity_score', 'log_affinity_score']  # Drop both 'score' and 'log_affinity_score'\n",
    "    else:\n",
    "        drop_cols = ['stars', 'user_id', 'business_id']  # Default columns to drop\n",
    "    \n",
    "    if cluster in [0, 2]:\n",
    "        X_train, y_train = prepare_data(train_df, cluster, drop_cols)\n",
    "        X_test, y_test = prepare_data(test_df, cluster, drop_cols)\n",
    "\n",
    "        cb_model = CatBoostRegressor(**large_catboost_params[cluster], verbose=False)\n",
    "        xgb_model = XGBRegressor(**large_xgb_params[cluster], objective='reg:squarederror', verbosity=0)\n",
    "        \n",
    "        cb_model.fit(X_train, y_train)\n",
    "        xgb_model.fit(X_train, y_train)\n",
    "        \n",
    "        cb_preds = cb_model.predict(X_test)\n",
    "        xgb_preds = xgb_model.predict(X_test)\n",
    "        \n",
    "        # Applying best weight ratio and clipping\n",
    "        #  if cluster == 0 else (0.65 * cb_preds + 0.35 * xgb_preds)\n",
    "        best_ratio, best_rmse = find_best_weight(cluster, X_train, y_train, X_test, y_test)\n",
    "        final_preds = (best_ratio * cb_preds + (1 - best_ratio) * xgb_preds)\n",
    "        final_preds = np.clip(final_preds, 1, 5)\n",
    "\n",
    "    elif cluster in [1]:\n",
    "        if pd.isnull(train_df['score']).any() or pd.isnull(test_df['score']).any():\n",
    "            drop_cols.append('score')\n",
    "\n",
    "        X_train, y_train = prepare_data(train_df, cluster, drop_cols)\n",
    "        X_test, y_test = prepare_data(test_df, cluster, drop_cols)\n",
    "        \n",
    "        # model = ElasticNet(**small_ES_params[cluster])\n",
    "        model = ElasticNetCV(l1_ratio=[.1, .5, .7, .9, .95, .99, 1], alphas=np.logspace(-6, 2, 100), cv=5, random_state=42)\n",
    "        model.fit(X_train, y_train)\n",
    "        final_preds = model.predict(X_test).clip(1, 5)\n",
    "\n",
    "    elif cluster in [3]:\n",
    "        test_size = 0.25  \n",
    "        X_train, y_train = prepare_data(train_df, cluster, drop_cols)\n",
    "        X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=test_size, random_state=42)\n",
    "        X_test, y_test = prepare_data(test_df, cluster, drop_cols)\n",
    "        model = CatBoostRegressor(**large_catboost_params[cluster], verbose=False)\n",
    "        model.fit(X_train, y_train, eval_set=(X_val, y_val), early_stopping_rounds=30)\n",
    "        final_preds = model.predict(X_test).clip(1, 5)\n",
    "        \n",
    "    elif cluster in [4, 5, 6, 7, 8]:\n",
    "        test_size = 0.1\n",
    "        X_train, y_train = prepare_data(test_df, cluster, drop_cols)\n",
    "        X_test, y_test = prepare_data(test_df, cluster, drop_cols)\n",
    "        X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=test_size, random_state=42)\n",
    "        \n",
    "        model = CatBoostRegressor(**(large_catboost_params[cluster] if cluster in [3, 4] else medium_catboost_params[cluster]), verbose=False)\n",
    "        model.fit(X_train, y_train, eval_set=(X_test, y_test), early_stopping_rounds=10)\n",
    "        final_preds = model.predict(X_test).clip(1, 5)\n",
    "        \n",
    "    # merge a prediction df\n",
    "    id_and_stars = test_df[test_df['Cluster'] == cluster][['user_id', 'business_id', 'stars']]\n",
    "    predictions_df = pd.DataFrame(final_preds, index=id_and_stars.index, columns=['predicted_stars'])\n",
    "    results_df = pd.concat([id_and_stars, predictions_df], axis=1)\n",
    "    \n",
    "    rmse = np.sqrt(mean_squared_error(y_test, final_preds))\n",
    "    # rmse = np.sqrt(mean_squared_error(results_df['stars'], results_df['predicted_stars']))\n",
    "    return results_df, rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e92f82d-b26a-4837-bf56-ca512c88ac8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "3757e1ad-a1be-4132-a686-a0b55b773538",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE from each cluster: {2: 0.8709090833833981, 1: 0.9591603171535215, 3: 0.9515668812006262, 4: 0.8836018307304754, 5: 0.42056581286741207, 6: 0.5313983468603477, 7: 0.38217155591620017, 8: 0.8422656547470009}\n"
     ]
    }
   ],
   "source": [
    "clusters_rmse = {}\n",
    "all_results = pd.DataFrame()\n",
    "\n",
    "# for cluster in range(9):  # 假定有0到8个cluster\n",
    "for cluster in [2,1,3,4,5,6,7,8]:\n",
    "    result_df, rmse = train_model(cluster, train_df, val_df)\n",
    "    clusters_rmse[cluster] = rmse\n",
    "    all_results = pd.concat([all_results, result_df])\n",
    "\n",
    "print(\"RMSE from each cluster:\", clusters_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "6e26b767-3f46-4dce-90d3-e6e28cd8a3d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall RMSE across all clusters: 0.8478491926564184\n"
     ]
    }
   ],
   "source": [
    "overall_rmse = np.sqrt(mean_squared_error(all_results['stars'], all_results['predicted_stars']))\n",
    "print(\"Overall RMSE across all clusters:\", overall_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a7dbe4-2e06-4f7b-9b23-cfd6f3af788e",
   "metadata": {},
   "outputs": [],
   "source": [
    "3: 0.9291631952891815, 4: 0.9611976256185978}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd07b54d-42a7-4d56-8d26-0cdb75d1c8e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7efa9e5c-69f0-45e1-ad0c-cb313431e047",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fbe6153-788d-4a32-b9bf-7bc6f521fa8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "811457fb-d37b-486d-83ae-031b4bf11a84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf54ed5-a5b2-45ce-b668-5456a0291a08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a0a193e6-4690-4ca6-929a-4abd0dd3ef3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 50/50 [00:09<00:00,  5.54trial/s, best loss: 0.8698640657701803]\n",
      "Cluster 2 - 最佳参数: {'colsample_bytree': 0.682741441716062, 'gamma': 0.07673842656496505, 'learning_rate': 0.1232008650603654, 'max_depth': 0, 'min_child_weight': 4, 'n_estimators': 0, 'subsample': 0.8765024979544617}\n",
      "Cluster 2 - 最终测试集上的RMSE: 3.4439417837227126\n"
     ]
    }
   ],
   "source": [
    "from hyperopt import hp, fmin, tpe, Trials, STATUS_OK\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def optimize_cluster2(train_df, val_df):\n",
    "    # 定义搜索空间\n",
    "    space = {\n",
    "        'max_depth': hp.choice('max_depth', [2, 3, 4]),\n",
    "        'n_estimators': hp.choice('n_estimators', range(270, 310, 10)),\n",
    "        'learning_rate': hp.uniform('learning_rate', 0.1, 0.15),\n",
    "        'subsample': hp.uniform('subsample', 0.85, 0.92),\n",
    "        'colsample_bytree': hp.uniform('colsample_bytree', 0.5, 0.8),  # 增加colsample_bytree进行搜索\n",
    "        'gamma': hp.uniform('gamma', 0, 1),  # 增加gamma进行搜索\n",
    "        'min_child_weight': hp.choice('min_child_weight', range(1, 6))  # 增加min_child_weight进行搜索\n",
    "    }\n",
    "\n",
    "    # 目标函数\n",
    "    def objective(params):\n",
    "        model = XGBRegressor(\n",
    "            objective='reg:squarederror',\n",
    "            max_depth=params['max_depth'],\n",
    "            learning_rate=params['learning_rate'],\n",
    "            n_estimators=params['n_estimators'],\n",
    "            subsample=params['subsample'],\n",
    "            colsample_bytree=params['colsample_bytree'],\n",
    "            gamma=params['gamma'],\n",
    "            min_child_weight=params['min_child_weight']\n",
    "        )\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_val)\n",
    "        rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n",
    "        return {'loss': rmse, 'status': STATUS_OK}\n",
    "\n",
    "    # 准备数据\n",
    "    train_cluster_data = train_df[train_df['Cluster'] == 2]\n",
    "    X_train = train_cluster_data.drop(columns=['stars', 'user_id', 'business_id'])\n",
    "    y_train = train_cluster_data['stars']\n",
    "\n",
    "    val_cluster_data = val_df[val_df['Cluster'] == 2]\n",
    "    X_val = val_cluster_data.drop(columns=['stars', 'user_id', 'business_id'])\n",
    "    y_val = val_cluster_data['stars']\n",
    "\n",
    "    # 使用hyperopt进行优化\n",
    "    trials = Trials()\n",
    "    best = fmin(\n",
    "        fn=objective,\n",
    "        space=space,\n",
    "        algo=tpe.suggest,\n",
    "        max_evals=50,  # 这里设置迭代次数\n",
    "        trials=trials\n",
    "    )\n",
    "\n",
    "    # 实例化和训练最佳模型\n",
    "    best_model = XGBRegressor(\n",
    "        objective='reg:squarederror',\n",
    "        max_depth=int(best['max_depth']),\n",
    "        learning_rate=best['learning_rate'],\n",
    "        n_estimators=int(best['n_estimators']),\n",
    "        subsample=best['subsample'],\n",
    "        colsample_bytree=best['colsample_bytree'],\n",
    "        gamma=best['gamma'],\n",
    "        min_child_weight=int(best['min_child_weight'])\n",
    "    )\n",
    "    best_model.fit(X_train, y_train)\n",
    "    y_pred = best_model.predict(X_val)\n",
    "    final_rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n",
    "\n",
    "    # 输出结果\n",
    "    print(f\"Cluster 2 - 最佳参数: {best}\")\n",
    "    print(f\"Cluster 2 - 最终测试集上的RMSE: {final_rmse}\")\n",
    "    return best_model, final_rmse\n",
    "\n",
    "# 调用函数时，传入训练数据和验证数据\n",
    "best_model, final_rmse = optimize_cluster2(train_df, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03a651f-6769-4a70-8a28-8d1d68ddf1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "    0: {'learning_rate': 0.04246778091879101, 'max_depth': 6, 'n_estimators': 491, 'subsample': 0.8689},\n",
    "    2: {'learning_rate': 0.12350751460907078, 'max_depth': 3, 'n_estimators': 288, 'subsample': 0.8880},"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
