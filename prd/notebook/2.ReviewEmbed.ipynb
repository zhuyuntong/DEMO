{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4318f1f-f247-49ba-aa1f-98befb68106d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/20 15:28:53 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import time\n",
    "import csv\n",
    "import math\n",
    "import numpy as np\n",
    "import pyspark\n",
    "from pyspark import SparkContext, SparkConf\n",
    "import json\n",
    "from operator import add\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from node2vec import Node2Vec as n2v\n",
    "import networkx as nx\n",
    "from hashlib import md5\n",
    "import pickle\n",
    "import os\n",
    "import pandas as pd\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 100)\n",
    "pd.set_option('display.width', 100)\n",
    "pd.set_option(\"display.precision\", 2)\n",
    "\n",
    "\n",
    "from utils import create_category_md5_mapping, integrate_mapping_user_bus_cat_data, dataframe_to_rdd_dict, analyze_top_business_categories, analyze_top_categories\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.mllib.recommendation import ALS, MatrixFactorizationModel, Rating\n",
    "\n",
    "def initialize_spark_context(APP_NAME=\"Train: XGBModel\"):\n",
    "    # Spark配置项列表\n",
    "    SPARK_CONF = [\n",
    "        (\"spark.dynamicAllocation.enabled\", \"true\"),  # 启用动态资源分配\n",
    "        (\"spark.dynamicAllocation.maxExecutors\", \"10\"),  # 最大执行器数量\n",
    "        (\"spark.executor.memory\", \"3g\"),  # 每个执行器的内存\n",
    "        (\"spark.executor.cores\", \"2\"),  # 每个执行器的CPU核心数\n",
    "        (\"spark.executor.memoryOverhead\", \"3000\"),  # 执行器内存开销\n",
    "        (\"spark.driver.memory\", \"4g\"),  # 驱动程序的内存\n",
    "        (\"spark.driver.maxResultSize\", \"2g\"),  # 驱动程序的最大结果大小\n",
    "        (\"spark.python.worker.memory\", \"2g\"),  # Python工作进程的内存\n",
    "        (\"spark.sql.shuffle.partitions\", \"20\"),  # Shuffle操作的分区数\n",
    "        (\"spark.sql.sources.partitionOverWriteMode\", \"dynamic\"),  # 分区覆写模式\n",
    "        (\"spark.network.timeout\", \"600s\"),  # 网络超时设置\n",
    "        (\"spark.executor.heartbeatInterval\", \"120s\"),  # 执行器心跳间隔\n",
    "    ]\n",
    "\n",
    "    # 创建Spark配置\n",
    "    spark_conf = pyspark.SparkConf()\n",
    "    spark_conf.setAppName(APP_NAME)\n",
    "    spark_conf.setAll(SPARK_CONF)\n",
    "\n",
    "    # 创建SparkContext\n",
    "    sc = pyspark.SparkContext(conf=spark_conf)\n",
    "    sc.setLogLevel(\"ERROR\")  # 设置日志级别\n",
    "\n",
    "    return sc\n",
    "\n",
    "sc = initialize_spark_context()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f93e0f7-7df3-444e-8ff2-e7760843cc3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.5 MB 4.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from nltk) (1.4.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from nltk) (4.66.2)\n",
      "Collecting regex>=2021.8.3\n",
      "  Downloading regex-2024.4.16-cp38-cp38-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (776 kB)\n",
      "\u001b[K     |████████████████████████████████| 776 kB 16.8 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: regex, nltk\n",
      "Successfully installed nltk-3.8.1 regex-2024.4.16\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63bbaad7-8395-403a-9e91-b4c7320b1e01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE!\n"
     ]
    }
   ],
   "source": [
    "import better_features\n",
    "from better_features import FeatureProcessor, read_json_data, transform_user_data, transform_business_data, extract_review_data\n",
    "from datetime import datetime #add\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "def rdd_to_pandas(rdd):\n",
    "    return pd.DataFrame(rdd.collect(), columns=rdd.first().keys())\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "user_parsed_df = pd.read_csv('cache/user_df.csv') # parsed from users.json\n",
    "business_parsed_df = pd.read_csv('cache/business_df.csv') # parsed from business.json\n",
    "review_parsed_df = pd.read_csv('cache/review_df.csv') # parsed from business.json\n",
    "\n",
    "print(\"DONE!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3192673-358e-4980-8f0f-365a0aee720a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n"
     ]
    }
   ],
   "source": [
    "from better_features import FeatureProcessor, read_json_data, transform_user_data, transform_business_data,extract_review_data\n",
    "from utils import integrate_mapping_user_bus_cat_data\n",
    "from KMeans_user_cluster import KMeans_process_user_clusters\n",
    "\n",
    "\n",
    "feature_processor = FeatureProcessor(user_parsed_df, business_parsed_df, review_parsed_df)\n",
    "\n",
    "# Kmeans find clusters for user and biz, here are some mappings to be used\n",
    "user_clusters, clusters_important_cities, business_with_important_cat_df = KMeans_process_user_clusters(feature_processor.map_reviews_with_categories(), business_parsed_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b6eb73b-4288-49d7-9193-c6671aef7715",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## TRAIN DATA FEATURE PROCESSING HERE\n",
    "# train_data_file = '../yelp_combined.csv'\n",
    "# train_df = pd.read_csv(train_data_file)\n",
    "\n",
    "# data_folder_path = '../data/'\n",
    "# res_rdd = feature_processor.process_all_features(sc, train_df, data_folder_path, train_data_file)\n",
    "\n",
    "# final_df = rdd_to_pandas(res_rdd)\n",
    "# final_df = final_df.merge(user_clusters, on='user_id', how='left')\n",
    "\n",
    "# ## TEST DATA FEATURE PROCESSING HERE\n",
    "# test_data_file = '../yelp_true.csv'\n",
    "# test_df = pd.read_csv(test_data_file)\n",
    "\n",
    "# test_res_rdd = feature_processor.process_all_features(sc, test_df, data_folder_path, test_data_file)\n",
    "\n",
    "# val_df = rdd_to_pandas(test_res_rdd)\n",
    "# val_df = val_df.merge(user_clusters, on='user_id', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2b9ef9-901e-4273-8f56-cc4fe1261b20",
   "metadata": {},
   "source": [
    "## START"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "57325feb-864c-4d12-9f55-8a92f894ab00",
   "metadata": {},
   "outputs": [],
   "source": [
    "review_df= pd.read_csv('cache/review_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d1847d2a-fc6e-4cdf-9356-9cc6d66987db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.cli import download\n",
    "\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# download('en_core_web_sm')\n",
    "# nlp = spacy.load('en_core_web_sm', disable=[\"parser\", \"ner\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cdf3cd4b-54a2-4c57-8ae8-c29b4fbc1db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "# import inflect\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def normalize_lemmatize(words):\n",
    "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "    \n",
    "    return [lemmatizer.lemmatize(word.lower()) for word in words if word.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "76c5f5e7-564d-41fc-8ad6-ba0ae766ee24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_processed_with_spark(sample, sc):\n",
    "    # 直接从Pandas DataFrame创建RDD\n",
    "    rdd = sc.parallelize(sample[['business_id', 'text']].values.tolist())\n",
    "\n",
    "    # 定义转换函数\n",
    "    def process_row(row):\n",
    "        business_id, text = row  # 直接解包元组\n",
    "        words = nltk.word_tokenize(text)\n",
    "        processed_text = ' '.join(normalize_lemmatize(words))\n",
    "        return (business_id, processed_text)\n",
    "\n",
    "    processed_rdd = rdd.map(process_row)\n",
    "    return processed_rdd\n",
    "\n",
    "def get_review_processed_with_spark(processed_rdd, reviews_rdd, sc):\n",
    "    # 使用broadcast优化查找过程\n",
    "    processed_business_ids = sc.broadcast(set(processed_rdd.map(lambda x: x[0]).collect()))\n",
    "    \n",
    "    # 使用filter过滤reviews_rdd\n",
    "    reviews_processed_rdd = reviews_rdd.filter(lambda x: x['business_id'] in processed_business_ids.value)\n",
    "    return reviews_processed_rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e6321617-dd59-48cb-ac64-56bb6df00f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_rdd = get_processed_with_spark(review_df, sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "02fff95e-3dde-4867-b73a-1d061d9b9a7b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Make sure not overwrite, Already saved.\n",
    "try:\n",
    "    processed_rdd.saveAsTextFile('cache/processed_review_text')\n",
    "    print(\"Data saved successfully!\")\n",
    "except Exception as e:\n",
    "    print(\"Error saving data: \", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bdff8723-1db1-48e8-882e-30dcd9c2971e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('iKMLsX1Je7P3wAOEc9scDg',\n",
       "  'i have been itching to get to origin for month now after a friend of mine had gone and raved about the food i dug the concept tapa with a twist this is definitely not your traditional spanish tapa at origin you sample small plate with asian italian and latin influence i did find the menu overwhelming but our server wa more than happy to help steer the meal so that similar and complimentary flavour were eaten together it wa tough to narrow it down a we would have been happy with just about anything on the menu we started with the tostones smashed flattened and plantain with guacamole on the side the plantain were very well seasoned with generous use of kosher salt and a light dusting of curry powder next up were the deviled egg with smoked bacon and gremolata and the smoked cod croquette with saffron aioli both were a big hit at the table disappearing in a flash i preferred the croquette of the two but i did exactly not eat my share of the deviled egg either those scrumptious bite devoured we ordered another bottle of wine three cheer for monday night dinner with lot of wine and anticipated the arrival of more delectable plate while we waited we marveled at the strangeness of the cutlery provided the only way to balance your knife wa blade up that seemed dangerous especially after the third glass of cabernet sauvignon our next dish appeared and we dove right back in carefully so a not to cut ourselves on our upturned knife a order from the mozzarella bar bufala mozzarella with pear rosemary oil pine nut that wa drizzled with honey and placed on a crunchy toasted slice of calabrese bread next up wa the bangkok beef salad with peanut mint mango fried shallot and a sweet and sour dressing there wa a moment of hesitation while we all tried to dance around the fact that splitting this thing among the three of u might get violent in the end however we shared well our kindergarten teacher would be so proud this wa an exceptionally complicated dish but it wa done to perfection the beef wa tender and pink the mango were ripe and slippery the shallot and peanut added the crunch the dish needed and the cilantro added the wonderful bright note at the end it should be noted that my friend said it wa the third time she had this particular dish and it wa exactly the same every time that is a sign of a great restaurant our final round of food wa to arrive and we were practically giddy with excitement we had decided on the chinois duck with pickled cucumber hoisin and sriracha sauce on a chive pancake the curried shrimp with naan and the black cod with soba noodle and a ginger vinaigrette we knew each dish would be good but did know how good we had spent the first half of the meal happily eating away and we were by no mean disappointed with our choice but these last three dish absolutely blew them all out of the water the duck wa crispy and i must say substantial the kitchen doe not scrimp on the portion at all with this dish the sweet hoisin sauce wa a great compliment to the duck and the heat of the sriracha a definite at origin next up we attacked the curried shrimp which wa intensely flavoured and perfectly spicy the soft aromatic naan bread sopped up the spicy broth that wa left after devouring the plump juicy shrimp another dish that hit it mark finally and kind of sadly we came to our last dish the black cod black cod is one my favourite fish and i have it a fair bit when i wa out in vancouver earlier this year i had it almost once a day but this black cod might be the best i had ever it wa flaky tender and moist the miso glaze wa pleasantly salty and the skin most importantly wa nice and crispy the soba noodle were served with the ginger vinaigrette and by the time we were done there wa literally nothing left on the plate in all likelihood the best dish of the night though the three of u never could decide on a clear winner in that category all in all i would definitely recommend origin to anyone who ha some reasonably adventurous taste bud and who is willing to share there is one composed plate that make a meal a burger combo with spanish fry and a float i have no doubt it delicious but it seems kind of beside the point of a restaurant like this go with friend go on a date go with family it not a cheap night out but it is most definitely money well spent')]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread \"serve RDD 14\" java.net.SocketTimeoutException: Accept timed out\n",
      "\tat java.net.PlainSocketImpl.socketAccept(Native Method)\n",
      "\tat java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:409)\n",
      "\tat java.net.ServerSocket.implAccept(ServerSocket.java:560)\n",
      "\tat java.net.ServerSocket.accept(ServerSocket.java:528)\n",
      "\tat org.apache.spark.security.SocketAuthServer$$anon$1.run(SocketAuthServer.scala:64)\n"
     ]
    }
   ],
   "source": [
    "processed_rdd.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b657ad-40b8-4672-8f82-cdf1a181e58e",
   "metadata": {},
   "source": [
    "## LOAD RDD ADN START HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a38a3b-f20b-4862-aa5a-6abc97ff5943",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load RDD here\n",
    "\n",
    "processed_rdd = sc.textFile('cache/processed_review_text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9d8d59-4ab7-4344-bf15-3faa2fea7ca5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557fe8c5-6c2d-4ce6-8001-5a790c7c6ca4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "161de5e6-ba1a-4052-ba17-cc650d92658c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import gensim.downloader as api\n",
    "# from gensim.models import Word2Vec\n",
    "# from gensim.models import KeyedVectors\n",
    "\n",
    "# # 加载预训练的Word2Vec模型\n",
    "# word_vectors = api.load(\"word2vec-google-news-300\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "43a6f242-7ed1-4dd5-8898-13e692a532eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def sentence_vector(sentence):\n",
    "#     words = sentence.split()\n",
    "#     if len(words) >= 1:\n",
    "#         return np.mean([model[word] for word in words if word in model], axis=0)\n",
    "#     else:\n",
    "#         return np.zeros(100)  # 假设词向量长度为100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c0c74d31-99a7-4540-a9ba-553d2297cde2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# review_df['sentence_vec'] = review_df['processed_text'].apply(sentence_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82dcfd50-17f1-4a3d-8f3a-6252fed08160",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ef592c-ae10-4dfe-915f-dace1cbf4245",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
